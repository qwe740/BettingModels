{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "from tqdm import tqdm # Optional: for progress bar\n",
    "import eloModel_Functions as elo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from database...\n",
      "Loaded 9817 games and 1420 returning production records.\n",
      "Calculating Elo ratings...\n",
      "\n",
      "Processing start of Season 2013...\n",
      "Regressed ratings for 0 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2014...\n",
      "Regressed ratings for 207 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2015...\n",
      "Regressed ratings for 215 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2016...\n",
      "Regressed ratings for 223 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2017...\n",
      "Regressed ratings for 227 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2018...\n",
      "Regressed ratings for 229 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2019...\n",
      "Regressed ratings for 234 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2020...\n",
      "Regressed ratings for 235 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2021...\n",
      "Regressed ratings for 236 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2022...\n",
      "Regressed ratings for 239 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2023...\n",
      "Regressed ratings for 242 teams using RP metric 'usage'.\n",
      "\n",
      "Processing start of Season 2024...\n",
      "Regressed ratings for 245 teams using RP metric 'usage'.\n",
      "Elo calculation complete.\n"
     ]
    }
   ],
   "source": [
    "# --- Main Execution ---\n",
    "conn = elo.connect_db()\n",
    "games_df, rp_df = elo.load_data(conn)\n",
    "\n",
    "final_elos, pre_game_elo_df = elo.run_elo_calculation(games_df, rp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Evaluation vs Closing Spread:\n",
      "  Mean Absolute Error (MAE): 165.0663\n",
      "  Correlation: 0.8351\n",
      "  Bias (Predicted - Actual): 25.5877\n",
      "\n",
      "Final Elo Ratings (Top 25):\n",
      "               team    final_elo\n",
      "134          Oregon  1831.428752\n",
      "101         Georgia  1779.496702\n",
      "68       Notre Dame  1770.926301\n",
      "108           Texas  1767.743056\n",
      "90       Ohio State  1754.255911\n",
      "98       Penn State  1727.121383\n",
      "115     Boise State  1714.965479\n",
      "48              SMU  1705.695018\n",
      "113         Alabama  1691.604862\n",
      "17         Ole Miss  1690.904520\n",
      "100         Clemson  1680.801697\n",
      "140       Tennessee  1654.734763\n",
      "30   South Carolina  1646.720340\n",
      "150   Arizona State  1632.173257\n",
      "132            Iowa  1618.422414\n",
      "76         Michigan  1606.233608\n",
      "46            Miami  1606.015455\n",
      "146      Louisville  1599.959673\n",
      "78         Missouri  1597.452676\n",
      "111             BYU  1596.454939\n",
      "139             LSU  1591.827288\n",
      "66       Iowa State  1590.956168\n",
      "118        Marshall  1587.602060\n",
      "11             UNLV  1583.155726\n",
      "44     Kansas State  1581.665952\n"
     ]
    }
   ],
   "source": [
    "# --- Merge Elo back into games data ---\n",
    "# Use game_id for robust merging\n",
    "games_with_elo = pd.merge(games_df, pre_game_elo_df[['game_id', 'home_pregame_elo', 'away_pregame_elo']],\n",
    "                            left_on='id', right_on='game_id', how='left')\n",
    "\n",
    "# --- Basic Evaluation / Next Steps ---\n",
    "# 1. Calculate Predicted Spread based on Elo\n",
    "games_with_elo['predicted_spread_elo'] = games_with_elo['away_pregame_elo'] - games_with_elo['home_pregame_elo'] + elo.HFA\n",
    "# Adjust for neutral site if needed (remove HFA)\n",
    "games_with_elo.loc[games_with_elo['neutral_site'] == 1, 'predicted_spread_elo'] -= elo.HFA\n",
    "\n",
    "# 2. Compare predicted_spread_elo with avg_closing_spread\n",
    "# Need to drop games where closing spread is missing for fair comparison\n",
    "eval_df = games_with_elo.dropna(subset=['avg_closing_spread', 'predicted_spread_elo']).copy()\n",
    "if not eval_df.empty:\n",
    "    mae = np.mean(np.abs(eval_df['predicted_spread_elo'] - eval_df['avg_closing_spread']))\n",
    "    correlation = eval_df['predicted_spread_elo'].corr(eval_df['avg_closing_spread'])\n",
    "    print(f\"\\nEvaluation vs Closing Spread:\")\n",
    "    print(f\"  Mean Absolute Error (MAE): {mae:.4f}\")\n",
    "    print(f\"  Correlation: {correlation:.4f}\")\n",
    "\n",
    "    # Check bias\n",
    "    bias = np.mean(eval_df['predicted_spread_elo'] - eval_df['avg_closing_spread'])\n",
    "    print(f\"  Bias (Predicted - Actual): {bias:.4f}\")\n",
    "else:\n",
    "    print(\"\\nCould not perform evaluation: No games with both predicted Elo spread and closing spread.\")\n",
    "\n",
    "# 3. Analyze Final Ratings (Optional)\n",
    "print(\"\\nFinal Elo Ratings (Top 25):\")\n",
    "final_ratings_df = pd.DataFrame(list(final_elos.items()), columns=['team', 'final_elo']).sort_values('final_elo', ascending=False)\n",
    "print(final_ratings_df.head(25))\n",
    "\n",
    "# 4. Save results (Optional)\n",
    "# games_with_elo.to_csv(\"games_with_pregame_elo.csv\", index=False)\n",
    "# final_ratings_df.to_csv(\"final_team_elos.csv\", index=False)\n",
    "\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
